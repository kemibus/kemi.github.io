---
title: "Class_projectpdf"
author: "Kemi"
date: "2024-12-02"
output: html_document
---


#Preliminatry idea for title
How Do Journalists Remember Themselves? Dominant Themes in Articles Commemorating Assassinated Nigerian Journalist, Dele Giwa 

# Summary of final project

This study examines four hundred and fifty four commemorative articles about assasinated Nigerian journalist, Dele Giwa, to identify the dominant themes. The study also analyzes differences in the commemoration during the military era (1987 to May 1999) and civilian (1999 to 2023).

# Research Questions

1.	What are the dominant themes in the articles commemorating Dele Giwa’s assassination?
2.	What are the differences in the commemoration of Dele Giwa between the military era (Oct 1987 to May 1999) and the civilian era (May 1999 to Oct 2023)?

# content analysis plan
The 454 articles were loaded into R, cleaned and prepared for analysis. The unit of analysis are news and opinion articles about Dele Giwa. The following analysis were run with the ultimate aim of identifying dominant themes:
Bigrams: The dataset was tokenized and then analyzed for bigrams to reveal the top phrases.
Topic modelling: Latent Dirichlet Allocation (LDA) was used to cluster words into topics. This will also be used to examine how the themes evolve over time (military vs. civilian era).
Sentiment analysis: The NRC dictionary will be used to analyze the overall tone of the articles. (Not sure about this yet because I think this won’t bring anything special)

# Coding scheme
Below is a coding scheme developed based on the literature and my familiarization with the data:

Bravery and Sacrifice: Mentions of Dele Giwa’s courage, investigative prowess, or ultimate sacrifice for truth and justice.

Calls for Justice: different expressions advocating for justice or accountability for his assassination.

Criticism of Authorities: Critical reflections on the failure of successive governments, law enforcement, or judiciary to solve the murder will fall under this category

Freedom of the Press: Themes highlighting press freedom and the risks journalists face in Nigeria.

Comparison of Eras: Contrasts between the military and civilian eras in terms of press freedom, accountability, or socio-political environment.

Role Model: Description of Dele Giwa as an icon of good journalism whose work inspires other journalists.



# Load libraries
```{r, warning=FALSE}
library(pdftools)
library(tidyverse)
library(stringr)
library(readxl)
library(rio)
library(tidytext)
library(dplyr)
library(tidyr)
```

## link to dataset on GitHub
```{r}
#link - https://github.com/kemibus/kemi_new/blob/main/dele.PDF
```

# Read text from a PDF file
```{r}
# Specify the file path
#filepath <- "~/GitHub/kemi_new/dele.PDF"
#Read pdf and coverting it into text
text <- pdf_text("~/GitHub/kemi_new/dele.PDF") 
#write to a text file
writeLines (text, "~/GitHub/kemi_new/dele.text")

```


# Split the text to have one article per file
```{r}

#Read the text from the PDF
#dele_text <- pdf_text("~/GitHub/kemi_new/dele.PDF")
file_path <- ("~/GitHub/kemi_new/dele.text")
text_data <- read_lines(file_path)
# Combine all the pages into one text 
text_combined <- paste(text_data, collapse = "\n")

# Split the text by the "End of Document" phrase 
dele_split <- strsplit(text_combined, "End of Document")[[1]]

# Set the output directory for saving the split files
output_dir <- "~/GitHub/kemi_new/dele_extracted"

# Write each section to a new text file
for (i in seq_along(dele_split)) {
  output_file <- file.path(output_dir, paste0("dele_extracted", i, ".txt"))
  writeLines(dele_split[[i]], output_file)
}

# Print a message to confirm how many files were created
cat("Files created:", length(dele_split), "\n")

```

## Extract lines to create index
```{r}
dele_index <- read_lines("~/GitHub/kemi_new/dele_extracted/dele_extracted1.txt")
extracted_lines <-dele_index[16:1504]
cat(head(extracted_lines, 7), sep = "\n")
extracted_lines <- extracted_lines |> 
  as.data.frame()
```

## Build a final dataframe index
## Add data from the excel sheet
```{r, warning=FALSE}

final_data <- rio::import("dele_list.XLSX") |> #with the new index
  janitor::clean_names() |> 
   mutate(date = as.Date(strptime(published_date, "%B %d, %Y"), "%Y-%m-%d")) |> 
  mutate(index = row_number()) |>
  mutate(year=year(date))
```

  

## Raw text compiler 
```{r, warning=FALSE}
# List all text files in the directory
files <- list.files("~/GitHub/kemi_new/dele_extracted", pattern="*.txt") |>
  as.data.frame() |> 
  rename(filename = 1) |> 
  mutate(
    # Create an index with the file name
    index = str_extract(filename, "\\d+"),
    index = as.numeric(index)
  )

# Join the file list to the final_data dataframe based on index
final_index <- final_data |>
  inner_join(files, c("index")) |>
  mutate(filepath = paste0("~/GitHub/kemi_new/dele_extracted/", filename))

# Display the head of the compiled dataframe with filenames and content
head(final_index)
```


## Text compiler
```{r, warning=FALSE,echo = FALSE, results = 'hide' }
# Define function to loop through each text file 

create_article_text <- function(row_value) {
  temp <- final_index %>%
    slice(row_value)
  
 
  temp_filename <- temp$filename
  
  articles_df_temp <- read_lines(temp$filepath) %>%
    as_tibble() %>%
    mutate(filename = temp_filename)
 
  articles_df <<- articles_df %>%
    bind_rows(articles_df_temp)
}

articles_df <- tibble()
row_values <- 1:nrow(final_index)

lapply(row_values, create_article_text)


# Clean up articles_df and join to index dataframe
articles_df <- articles_df %>%
  select(filename, sentence=value) %>%
  inner_join(final_index)

#After viewing articles_df, I see 1505 lines from the index that I don't need. Cutting them 

articles_df <- articles_df %>%
  slice(-c(1:1505)) |> 
  #gets rid of blank rows
    filter(trimws(sentence) != "") 

write_csv(articles_df, "articles_df.csv")
head(articles_df, 10)
```



## Remove items in the bracket in final index
```{r, warning=FALSE,echo = FALSE, results = 'hide' }
final_index <- final_index %>%
  mutate(across(everything(), ~ str_remove_all(.x, "\\[.*?\\]|\\(.*?\\)|\\{.*?\\}")))
```


## Merge "Weekly Trust" into "Daily Trust", they are the same
```{r}
final_index <- final_index %>%
  mutate(publication_4 = str_replace(publication_4, "Weekly Trust", "Daily Trust"))
```

## Number of rows - 454
```{r}
nrow(final_index)
```

## Number of columns - 4
```{r}
ncol(final_index)
```


## Count the number of articles per year
```{r}
# Count the number of articles published per year and arrange by year
articles_per_year <- final_index %>%
  count(year) %>%
  filter(!is.na(year)) %>%
  arrange(year)

# View the count of articles per year
print(articles_per_year)
```

## Visualize articles per year in a table
```{r, warning=FALSE}
# Load the DT package
library(DT)
articles_per_year_table <- articles_per_year %>%
  arrange(year)  

# Create an interactive table
datatable(articles_per_year_table, 
          options = list(
            pageLength = 10,    # Number of rows per page
            autoWidth = TRUE,   # Automatically adjust column width
            searching = FALSE,  # Disable search bar
            ordering = TRUE     # Allow sorting of columns
          )) %>%
  formatStyle(
    'n', # Target the column 'n' (Number of Articles)
    backgroundColor = styleInterval(0, c('#f2f2f2', '#d9f2d9')), # Add alternating row colors
    fontWeight = 'bold'  # Make the font bold
  )
```


## Identify the top ten years with the most publications
```{r}
top_10 <- articles_per_year %>%
  top_n(10, n) %>%
  arrange(desc(n))
```


## Visualize top 10
```{r, warning=FALSE}
top_10_plot <- top_10 %>%
  ggplot(aes(x = reorder(factor(year), n), y = n)) +
  geom_bar(stat = "identity", fill = "#69b3a2", color = "black", width = 0.7) +
  geom_text(aes(label = n), vjust = -0.2, size = 3.5, fontface = "bold", color = "black") +
  labs(
    title = "Top 10 Years by Number of Articles",
    subtitle = "Date Created: Dec 7, 2024",
    caption = "Visualization by Kemi Busari",
    x = "Year",
    y = "Number of Articles"
  ) +
  theme_minimal(base_size = 14) +
  theme(
    plot.title = element_text(face = "bold", size = 16, hjust = 0.5),
    plot.subtitle = element_text(size = 12, hjust = 0.5),
    plot.caption = element_text(hjust = 0, size = 10),
    axis.text.x = element_text(face = "bold", size = 12),
    axis.title = element_text(face = "bold")
  )

# Print the plot
print(top_10_plot)
```


## count the number of articles in military era(1986 to 1999)
```{r}
articles_1986_1999 <- final_index %>%
  filter(year >= 1986 & year <= 1999) %>%  
  count(year)
```
#Only 14 articles were published during the military era


## count the number of articles in the democratic era(2000 to 2023)
```{r}
articles_2000_2023 <- final_index %>%
  filter(year >= 2000 & year <= 2023) %>%  
  count(year)  
total_articles_2000_2023 <- sum(articles_2000_2023$n)

# Print the yearly breakdown and total articles
print(articles_2000_2023)
print(total_articles_2000_2023)

```
### 439 articles were published in the democratic era (2000 - 2023)


## Count the number of articles published by each newspaper
```{r}
articles_per_newspaper <- final_index %>%
  count(publication_4) %>%  
  group_by(publication_4) %>%  
  summarise(total_articles = sum(n)) %>%  
  arrange(desc(total_articles)) 
```
44 news platforms published about Dele Giwa


## Extract top 10 newspapers
```{r}
top_10_newspapers <- articles_per_newspaper %>%
  arrange(desc(total_articles)) %>%
  slice(1:10)
```



## Visualize top 10
```{r}
top_10_newspapers_plot <- top_10_newspapers %>%
  ggplot(aes(x = reorder(publication_4, total_articles), y = total_articles)) +
  geom_bar(stat = "identity", fill = "#0073C2FF", color = "#005f99", width = 0.7) +
  geom_text(aes(label = total_articles), vjust = -0.3, size = 4, fontface = "bold", color = "black") +
  labs(
    title = "Top 10 Newspapers by Number of Articles",
    subtitle = "Date Created: Dec 7, 2024",
    caption = "Visualization by Kemi Busari",
    x = "Newspaper",
    y = "Number of Articles"
  ) +
  theme_minimal(base_size = 14) +
  theme(
    plot.title = element_text(face = "bold", size = 16, hjust = 0.5),
    plot.subtitle = element_text(size = 12, hjust = 0.5),
    plot.caption = element_text(hjust = 0, size = 10),
    axis.text.x = element_text(angle = 45, hjust = 1, vjust = 1, size = 10),
    axis.title = element_text(face = "bold")
  ) +
  # Adjust the y-axis limit to ensure the highest bar and its label are visible
  ylim(0, max(top_10_newspapers$total_articles) * 1.1)

# Print the plot
print(top_10_newspapers_plot)
```


## bigrams
```{r, warning=FALSE}
# Load necessary libraries
library(dplyr)
library(tidytext)
library(stringr)
# Process bigrams
bigrams <- articles_df %>% 
  select(sentence) %>% 
  mutate(
    sentence = str_squish(sentence),                      
    sentence = tolower(sentence),
    sentence = str_replace_all(sentence, c(
      "copyright" = "",
      "new york times" = "",
      "publication" = "",
      "www.alt" = "",
      "http" = ""
    ))
  ) %>% 
  unnest_tokens(bigram, sentence, token = "ngrams", n = 2) %>% 
  separate(bigram, c("word1", "word2"), sep = " ") %>% 
  filter(!word1 %in% stop_words$word) %>%                 # Filter out stop words
  filter(!word2 %in% stop_words$word) %>% 
  count(word1, word2, sort = TRUE) %>% 
  filter(!is.na(word1) & !is.na(word2))   

# Ensure `stop_words` data is available
data("stop_words")

# Define the expanded remove pattern for unwanted terms
remove_pattern <- paste(
  "title", "pages", "publication date", "publication subject", "publication type", 
  "issn", "language of publication: english", "document url", "copyright", "news", "lenght", "words", "load", "date", "live updates", "updates assembly", "polling", "units", "total votes",
  "service last updated", "database", "start of article", "rights reserved", 
  "local assembly", "local government", "ups assembly", "live ups", "elections", 
  "accredited voters", "governorship", "registered", "total", "votes", "cast", 
  "valid", "rejected", "polling units", "electoral commission", 
  "progressives congress", "independent", "inec", "presidential", "proquest document id", 
  "classification", "illustration", "caption", "[0-9.]+", "_ftn", "[_]+", "aaa", 
  "jcr:fec", "___________________", "rauchway", "keynes's", "language", "english", 
  "length words", "global media", "allafrica global", "syndigate media", 
  "peoples democratic", "party pdp", "returning officer", "akwa ibom", 
  "managing director", "publishing limited", "sun publishing", "sun nigeria", "party pdp", sep = "|"
)

# Process bigrams again with the updated remove pattern
bigrams <- articles_df %>% 
  select(sentence) %>% 
  mutate(
    sentence = str_squish(sentence),                      # Remove extra spaces
    sentence = tolower(sentence),                         # Convert to lowercase
    sentence = str_replace_all(sentence, remove_pattern, ""), # Remove unwanted terms
    sentence = str_replace_all(sentence, "- ", ""),       # Remove trailing hyphens
    sentence = str_replace_all(sentence, "\\b[a-zA-Z]\\b", "") # Remove single characters
  ) %>% 
  unnest_tokens(bigram, sentence, token = "ngrams", n = 2) %>% 
  separate(bigram, c("word1", "word2"), sep = " ") %>% 
  filter(!word1 %in% stop_words$word) %>%                 # Filter out stop words
  filter(!word2 %in% stop_words$word) %>% 
  filter(!word1 %in% remove_pattern) %>% 
  count(word1, word2, sort = TRUE) %>% 
  filter(!is.na(word1) & !is.na(word2))                   # Filter out NAs

# Display the processed bigrams
print(bigrams)

```


## Top 20 bigrams
```{r}
top_bigrams <- bigrams %>%
  arrange(desc(n)) %>%  
  head(20)              
print(top_bigrams)
```


## Visualize top 20 bigrams
```{r}
# Ensure the required libraries are loaded
ggplot(top_bigrams, aes(x = reorder(paste(word1, word2, sep = " "), n), y = n)) +
  geom_col(fill = "steelblue") + 
  coord_flip() +  
  labs(
    title = "Top bigrams from articles",
    caption = "n=454 articles. Visualization by Kemi Busari. Dec 1, 2024",
    x = "Phrases",
    y = "Count of phrases"
  ) +
  theme_minimal(base_size = 14) +
  theme(
    plot.title = element_text(hjust = 0.5, face = "bold", size = 16),
    plot.caption = element_text(hjust = 0, face = "italic", size = 10),
    axis.title.x = element_text(face = "bold"),
    axis.title.y = element_text(face = "bold")
  )
```


# Sentiment Analysis

```{r, warning=FALSE}
library(textdata)
library(quanteda)
```

## Tokenize text
```{r}
text_tokenized <- articles_df %>% 
  select(sentence) %>% 
  unnest_tokens(word, sentence)
```


## Filter tokenized text
```{r}
text_tokenized <- articles_df %>% 
  select(sentence) %>% 
  mutate(sentence = str_replace_all(sentence, "- ", "")) %>% 
  unnest_tokens(word, sentence) %>% 
  filter(!word %in% stop_words$word) %>% 
  filter(word != "stories_corpus") %>%
  filter(!grepl('[0-9]', word))
```


## Word count
```{r}
text_word_ct <- text_tokenized %>%
  count(word, sort=TRUE)
text_word_ct
```


## Load NRC lexicon
```{r}
nrc_sentiments <- get_sentiments("nrc")

nrc_sentiments %>% count(sentiment)

nrc_sentiments %>% 
  group_by(word) %>% 
  count() %>% 
  arrange(desc(n)) %>% 
  distinct()
```

## Count Overall Sentiment with NRC
```{r}

sentiments_all <- text_tokenized %>%
  inner_join(nrc_sentiments) 

sentiments_all %>% 
  group_by(word) %>% 
    count(sentiment) %>% 
  arrange(desc(n))
```

```{r}
sentiments_all <- text_tokenized %>%
  inner_join(nrc_sentiments) %>%
  count(sentiment, sort = TRUE) %>% 
  mutate(pct_total =round(n/sum(n), digits=2))

sentiments_all
```



## Create ggplot
```{r}
# Load necessary libraries
library(ggplot2)
library(dplyr)

# Plot the sentiment analysis results
nrc_plot <- sentiments_all %>% 
  ggplot(aes(x = reorder(sentiment, n), y = n, fill = sentiment)) + 
  geom_bar(stat = "identity", position = "dodge") +  
  theme(legend.position = "none") +  
  labs(
    title = "Total Sentiment in Articles About Dele Giwa",
    subtitle = " ",
    caption = "NRC Sentiment Analysis. Graphic by Kemi Busari, Dec 1, 2024",
    x = "Total Sentiment Score",
    y = "Score"
  ) +
  theme_minimal(base_size = 14) +
  theme(
    plot.title = element_text(face = "bold", size = 16),
    plot.caption = element_text(hjust = 0, size = 10)
  ) +
  scale_fill_brewer(palette = "Set3") +
  coord_flip()  

# Print the plot
print(nrc_plot)
```


# Group by sentiment and word, then count the occurrences
```{r}
# Load necessary libraries
library(dplyr)

# Join the tokenized text with sentiment data
sentiments_all <- text_tokenized %>%
  inner_join(nrc_sentiments)

# Group by sentiment and word, then count the occurrences
top_words_per_sentiment <- sentiments_all %>%
  count(sentiment, word) %>%
  arrange(sentiment, desc(n)) %>%
  group_by(sentiment) %>%
  slice_max(n, n = 5, with_ties = FALSE)  # Get top 5 words for each sentiment

# View the result
print(top_words_per_sentiment)
```


## Extract top two words in each sentiment
```{r}
# Create the plot showing only the top 3 words per sentiment
sentiment_plot <- top_words_per_sentiment %>%
  group_by(sentiment) %>%
  top_n(2, n) %>%  # Select the top 3 words for each sentiment
  ungroup() %>%
  ggplot(aes(x = reorder(word, n), y = n, fill = sentiment)) +
  geom_bar(stat = "identity", show.legend = FALSE) +  # Create bar chart
  facet_wrap(~sentiment, scales = "free_y", ncol = 2) +  # Facet by sentiment
  labs(
    title = "Top 2 Words Associated with Each Sentiment",
    subtitle = "Top 2 Words in each sentiment",
    x = "Word",
    y = "Frequency",
    caption = "Visualization by Kemi Busari"
  ) +
  coord_flip() +  # Flip the axes for readability
  theme_minimal(base_size = 14) +
  theme(
    plot.title = element_text(face = "bold", size = 16, hjust = 0.5),
    plot.subtitle = element_text(size = 12, hjust = 0.5),
    plot.caption = element_text(hjust = 0, size = 10),
    axis.title = element_text(face = "bold"),
    strip.text = element_text(face = "bold")
  )

# Print the plot
print(sentiment_plot)
```






## Check the words in some of the sentiments

# Positive
```{r}
nrc_positive <- nrc_sentiments %>%
  filter(sentiment == "positive")
dele_positive <- text_tokenized %>%
  inner_join(nrc_positive) %>%
  count(word, sort = TRUE)
dele_positive

```

# Trust
```{r}
#Trust
nrc_trust <- nrc_sentiments %>%
  filter(sentiment == "trust")
dele_trust <- text_tokenized %>%
  inner_join(nrc_trust) %>%
  count(word, sort = TRUE)
dele_trust
```


# Negative
```{r}
nrc_negative <- nrc_sentiments %>%
  filter(sentiment == "negative")
dele_negative <- text_tokenized %>%
  inner_join(nrc_negative) %>%
  count(word, sort = TRUE)
dele_negative
```


# Fear
```{r}
nrc_fear <- nrc_sentiments %>%
  filter(sentiment == "fear")
dele_fear <- text_tokenized %>%
  inner_join(nrc_fear) %>%
  count(word, sort = TRUE)
dele_fear
```


# Anticipation
```{r}
nrc_anticipation <- nrc_sentiments %>%
  filter(sentiment == "anticipation")
dele_anticipation <- text_tokenized %>%
  inner_join(nrc_anticipation) %>%
  count(word, sort = TRUE)
dele_anticipation
```


# Anger
```{r}
nrc_anger <- nrc_sentiments %>%
  filter(sentiment == "anger")
dele_anger <- text_tokenized %>%
  inner_join(nrc_anger) %>%
  count(word, sort = TRUE)
dele_anger
```

## Sentiment during military era - 1986-1999
```{r}
# Filter the data for the years between 1986 and 1999
#military_sentiment <- articles_df %>%
  ##filter(year >= 1986 & year <= 1999) %>%  # Filter for the years 1986-1999
  #group_by(sentiment) %>%  # Group by sentiment
  #count(sentiment, sort = TRUE)  # Count the number of occurrences of each sentiment

# View the military_sentiment
#print(military_sentiment)
```

## Sentiment in the democratic era - 2000 -2023
```{r}
#military_sentiment <-  articles_df %>% 
  #filter(year >= 2000 & year <=2023)
```

Note: These two codes did not work still. Will keep working on them.


# Topic modelling

#Install and load packages 
```{r, warning=FALSE}
# install.packages("here")
# install.packages("tidytext")
# install.packages("quanteda")
# install.packages("tm")
# install.packages("topicmodels")
# install.packages("reshape2")
# install.packages("ggplot2")
# install.packages("wordcloud")
# install.packages("pals")
# install.packages("SnowballC")
# install.packages("lda")
# install.packages("ldatuning")
# install.packages("kableExtra")
# install.packages("DT")
# install.packages("flextable")
# install.packages("remotes")
# remotes::install_github("rlesur/klippy")
#install.packages("rio")
#install.packages("readtext")
#install.packages("formattable")


```


```{r include=FALSE}
# set options
options(stringsAsFactors = F)         # no automatic data transformation
options("scipen" = 100, "digits" = 4) # suppress math annotation
# load packages
here::here()
library(tidyverse)
library(tidytext)
library(rio)
library(readtext)
#topic modeling
library(quanteda)
library(tm)
library(topicmodels)
library(lda)
library(ldatuning)
# from tutorial packages
library(DT)
library(knitr) 
library(kableExtra) 
library(reshape2)
library(ggplot2)
library(wordcloud)
library(pals)
library(SnowballC)
library(flextable)
```


## Process into corpus object
```{r}
topic_data <- articles_df %>% 
  select(filename, sentence) %>% 
  as.data.frame() %>% 
  rename(doc_id = filename, text= sentence)

# load stopwords
english_stopwords <- readLines("https://slcladal.github.io/resources/stopwords_en.txt", encoding = "UTF-8")
# create corpus object
corpus <- Corpus(DataframeSource(topic_data))
# Preprocessing chain
processedCorpus <- tm_map(corpus, content_transformer(tolower))
processedCorpus <- tm_map(processedCorpus, removeWords, english_stopwords)
processedCorpus <- tm_map(processedCorpus, removePunctuation, preserve_intra_word_dashes = TRUE)
processedCorpus <- tm_map(processedCorpus, removeNumbers)
processedCorpus <- tm_map(processedCorpus, stemDocument, language = "en")
processedCorpus <- tm_map(processedCorpus, stripWhitespace)
```



```{r tm3a}
#DTM: rows correspond to the documents in the corpus. Columns correspond to the terms in the documents. Cells correspond to the weights of the terms. (Girder)
# compute document term matrix with terms >= minimumFrequency
minimumFrequency <- 5
DTM <- DocumentTermMatrix(processedCorpus, control = list(bounds = list(global = c(minimumFrequency, Inf))))
# have a look at the number of documents and terms in the matrix
dim(DTM)
# due to vocabulary pruning, we have empty rows in our DTM
# LDA does not like this. So we remove those docs from the
# DTM and the metadata
sel_idx <- slam::row_sums(DTM) > 0
DTM <- DTM[sel_idx, ]
topic_data <- topic_data[sel_idx, ]
#5 term minimum[1] 1387 3019
#5 term minimum[1] 308597 10339

``` 

## Topic proportions 

```{r, warning=FALSE}
# number of topics
# K <- 20
K <- 6
# set random number generator seed
set.seed(9161)
#Latent Dirichlet Allocation, LDA
topicModel2 <- LDA(DTM, K, method="Gibbs", control=list(iter = 1000, verbose = 25, alpha = 0.2))
tmResult <- posterior(topicModel2)
theta <- tmResult$topics
beta <- tmResult$terms
topicNames <- apply(terms(topicModel2, 10), 2, paste, collapse = " ")  # reset topicnames
```

## Number of articles
```{r}
# Step 1: Check dimensions
n_theta <- nrow(theta)
n_topicdata<- length(topic_data)

cat("Number of rows in theta: ", n_theta, "\n")
cat("Number of documents in textdata: ", n_topicdata, "\n")

# Check if textdata contains all the documents in theta
common_ids <- intersect(rownames(theta), topic_data$doc_id) # Assuming textdata has a 'doc_id' column

# Filter textdata to include only the documents present in theta
topicdata_filtered <- topic_data[topic_data$doc_id %in% common_ids, ]

# Check dimensions after filtering
n_topicdata_filtered <- nrow(topicdata_filtered)
cat("Number of documents in filtered textdata: ", n_topicdata_filtered, "\n")


# Align rownames of theta with filtered textdata
theta_aligned <- theta[rownames(theta) %in% topicdata_filtered$doc_id, ]

# Step 2: Combine data
full_data <- data.frame(theta_aligned, decade = topicdata_filtered)

# get mean topic proportions per decade
# topic_proportion_per_decade <- aggregate(theta, by = list(decade = textdata$decade), mean)
# set topic names to aggregated columns
colnames(full_data)[2:(K+1)] <- topicNames
# reshape data frame
vizDataFrame <- melt(full_data)
   
```

## Examine topic

```{r}
#enframe(): Converts a named list into a dataframe.
topics <- enframe(topicNames, name = "number", value = "text") %>% 
  unnest(cols = c(text)) 
  
topics
```

## Review the topics 
```{r}
theta2 <- as.data.frame(theta)

topic1 <- theta2 %>% 
  rownames_to_column(var = "file") |> # putting the rownames into a new column called file
  mutate(file = str_remove(file, "^X"),  # Remove leading 'X'
         line = str_extract(file, "(?<=\\.txt)\\.\\d+")) |>   # Extract number after .txt
  mutate(file = str_remove(file, "\\.\\d+$")) |> 
  rename(topic1 = '1') |> 
  top_n(20, topic1) |> 
  arrange(desc(topic1)) |>  
  select(file, line, topic1) 
```


## Add categories

```{r}
vizDataFrame <- vizDataFrame %>% 
  mutate(category = case_when(
    str_detect(variable, "govern presid countri nigeria militari polit peopl court nigerian nation") ~ "unresolved_case",
    str_detect(variable, "giwa dele journalist kill murder polic death bomb lago load-dat") ~ "assasination",
    str_detect(variable, "book journal school write ray read year univers nigerian stori") ~ "remenbrance",
    str_detect(variable, "page media nigeria newspap report nation editor right time news") ~ "nigerian_newspapers",
    str_detect(variable, "state vote elect pdp apc govern voter parti word unit") ~ "election",
    str_detect(variable, "peopl man god life day time good thing nigeria live
") ~ "memory",
    TRUE ~ NA_character_  
  ))

```





#Analysis 
```{r}
#Statistical analysis reveals a notable difference in article publication between the democratic and military eras, with significantly more articles published during the democratic era (439) compared to the military era (14). The peak year for publications about Giwa was 2010, with 53 articles, followed by 2023 (47), 2021 (45), 2018 (36), 2015 (34), and 2014 (27). The data indicates that over half (53%) of the articles were published in the top six years, spanning from 2010 to 2023. Additionally, the analysis identifies the newspaper with the most articles. As shown in Table 1, nine of the newspapers with the most articles are based in Lagos, where Giwa predominantly operated, while one, Daily Trust, is headquartered in Abuja.

#Figure 1: Top 10 newspapers by number of articles
 
#As shown in Figure 2, the dominant themes in the top bigrams show that the articles primarily focused on the circumstances surrounding Giwa’s death, the alleged perpetrator, his reputation as a respected journalist, the call for justice, and relationship with his associates. Phrases like “parcel bomb,” “letter bomb,” and “Giwa murder” reflect the prevailing narrative that Giwa was killed by a parcel bomb delivered to his home, which also underscores the ongoing discussion surrounding the unresolved case of his assassination. The annual commemoration of Giwa’s life is evident in the themes, with references such as “journalist Dele,” “Daily Times,” and “Watch Magazine” alluding to his time at various newspapers, including the one he founded, Newswatch Magazine. A key narrative associated with the phrase “journalist Dele” is the positive remembrance of Giwa. For instance, an article discussed an essay competition organized by the Etsako Central Local Government where he hails from. The competition was aimed at “immortalizing his name 37 years after his death.” The article notes that “late Dele Giwa, was a prominent journalist… (who)made significant contributions to the journalism profession and remains a symbol of integrity and courage in the face of adversity.”

#Figure 2: Top 20 bigrams in the articles
 
#Ibrahim Babangida, the Nigerian military head of state at the time of Giwa’s death, also emerged as a significant bigram. In the articles examined, Babangida’s name was often mentioned as the alleged killer and in connection to Giwa’s criticism of the military government, which earned him recognition as a fearless journalist. The mention was more prominent and targeted during the democratic era. In some articles, while Babangida was not explicitly named as the killer, he was blamed for failing to bring the perpetrators to justice. One such article, published in 2023, stated that “the only blame on Ibrahim Babangida was not fishing out his killers.” Calls for justice were a prominent theme in the articles, often conveyed through phrases like “press freedom,” “human rights,” “security agencies,” “national assembly,” and “Olusegun Obasanjo.” The prominence of “Olusegun Obasanjo” suggests a renewed call for justice following Nigeria’s return to democracy in 1999. Obasanjo, who became the first Nigerian president after years of military rule, presided over a period marked by increased demands for justice for Giwa. Articles discussing press freedom in Nigeria also referenced Giwa as an icon. This sentence “Dele Giwa paid the supreme price” was used to discuss the promises of the government in promoting press freedom in Nigeria in 2019 in one of the articles. Another article titled “Press freedom under attack” which discussed the state of press freedom in Nigeria in 2017 referenced Giwa’s assassination as the climax of press attack in Nigeria. Articles which feature the “Boko Haram” phrase indicate the continued mention of Dele Giwa’s death each time the country faces terrorist attacks. An instance of this was in 2013 when the Nigeria Postal Service (NIPOST) discovered a parcel bomb addressed to the Minister of Finance, Dr Ngozi Okonjo-Iweala. “The case of late Dele Giwa remains a sad reference point of such disguise of the evil intentions of the sponsors,” the article referenced.

#Table 1: Frequency of words in sentiment analysis
 
#The sentiment analysis of the dataset revealed a clear prevalence of positive sentiments, accounting for 22% of the total sentiment words. As shown in Figure 3, “journalist” and “president” were associated to this. Trust emerged as the second most frequent sentiment, representing 16% of the total, indicating a strong emphasis the government to unravel the cause of death. Negative sentiments followed closely at 14%, with fear (10%) and anticipation (9%) also notable, reflecting the presence of uncertainty around the death of Giwa. The emotions of anger and sadness were present but less dominant, contributing 7% and 7% respectively. Joy, surprise, and disgust appeared less frequently, with joy at 6%, surprise at 4%, and disgust at 4%. Overall, the findings suggest that the discourse surrounding the Giwa is predominantly positive, with significant levels of trust, yet also an undercurrent of fear, negative emotions, and anticipation. 

#Figure 3: Top two words associated with sentiment
 
#The topic modeling analysis identifies distinct themes in the dataset related to Dele Giwa’s death and Nigerian politics. The "unresolved_case" category captures ongoing discussions about the case, focusing on terms like "govern," "presid," "nigeria," and "court." The "assassination" theme highlights articles on Giwa’s killing, referencing "giwa," "dele," "journalist," and "death." "Remembrance" relates to articles on Giwa’s legacy, with keywords like "book," "school," and "nigerian." "Nigerian_newspapers" centers on media coverage, while "election" suggest discussions about Giwa’s death during elections. Lastly, "memory" encompasses personal reflections on his life.
```