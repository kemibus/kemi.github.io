---
title: "Class_projectpdf"
author: "Kemi"
date: "2024-12-02"
output: html_document
---


#Preliminatry idea for title
How Do Journalists Remember Themselves? Dominant Themes in Articles Commemorating Assassinated Nigerian Journalist, Dele Giwa 

# Summary of final project

This study examines four hundred and fifty four commemorative articles about assasinated Nigerian journalist, Dele Giwa, to identify the dominant themes. The study also analyzes differences in the commemoration during the military era (1987 to May 1999) and civilian (1999 to 2023).

# Research Questions

1.	What are the dominant themes in the articles commemorating Dele Giwa’s assassination?
2.	What are the differences in the commemoration of Dele Giwa between the military era (Oct 1987 to May 1999) and the civilian era (May 1999 to Oct 2023)?

# content analysis plan
The 454 articles were loaded into R, cleaned and prepared for analysis. The unit of analysis are articles about Dele Giwa. The following analysis were run with the ultimate aim of identifying dominant themes:
Bigrams: The dataset was tokenized and then analyzed for bigrams to reveal the top phrases.
Topic modelling: Latent Dirichlet Allocation (LDA) was used to cluster words into topics. This will also be used to examine how the themes evolve over time (military vs. civilian era).
Sentiment analysis: The Afinn dictionary will be used to analyze the overall tone of the articles. (Not sure about this yet because I think this won’t bring anything special)

# Coding scheme
Below is a coding scheme developed based on the literature and my familiarization with the data:

Bravery and Sacrifice: Mentions of Dele Giwa’s courage, investigative prowess, or ultimate sacrifice for truth and justice.

Calls for Justice: different expressions advocating for justice or accountability for his assassination.

Criticism of Authorities: Critical reflections on the failure of successive governments, law enforcement, or judiciary to solve the murder will fall under this category

Freedom of the Press: Themes highlighting press freedom and the risks journalists face in Nigeria.

Comparison of Eras: Contrasts between the military and civilian eras in terms of press freedom, accountability, or socio-political environment.

Role Model: Description of Dele Giwa as an icon of good journalism whose work inspires other journalists.



# Load libraries
```{r, warning=FALSE}
library(pdftools)
library(tidyverse)
library(stringr)
library(readxl)
library(rio)
library(tidytext)
library(dplyr)
library(tidyr)
```

## link to dataset on GitHub
```{r}
#link - https://github.com/kemibus/kemi_new/blob/main/dele.PDF
```

# Read text from a PDF file
```{r}
# Specify the file path
#filepath <- "~/GitHub/kemi_new/dele.PDF"
#Read pdf and coverting it into text
text <- pdf_text("~/GitHub/kemi_new/dele.PDF") 
#write to a text file
writeLines (text, "~/GitHub/kemi_new/dele.text")

```


# Split the text to have one article per file
```{r}

#Read the text from the PDF
#dele_text <- pdf_text("~/GitHub/kemi_new/dele.PDF")
file_path <- ("~/GitHub/kemi_new/dele.text")
text_data <- read_lines(file_path)
# Combine all the pages into one text 
text_combined <- paste(text_data, collapse = "\n")

# Split the text by the "End of Document" phrase 
dele_split <- strsplit(text_combined, "End of Document")[[1]]

# Set the output directory for saving the split files
output_dir <- "~/GitHub/kemi_new/dele_extracted"

# Write each section to a new text file
for (i in seq_along(dele_split)) {
  output_file <- file.path(output_dir, paste0("dele_extracted", i, ".txt"))
  writeLines(dele_split[[i]], output_file)
}

# Print a message to confirm how many files were created
cat("Files created:", length(dele_split), "\n")

```

## Extract lines for index
```{r}
dele_index <- read_lines("~/GitHub/kemi_new/dele_extracted/dele_extracted1.txt")
extracted_lines <-dele_index[16:1504]
cat(head(extracted_lines, 7), sep = "\n")
extracted_lines <- extracted_lines |> 
  as.data.frame()
```

## Build a final dataframe index

```{r}
# Step 1: Trim spaces and detect rows with titles and dates
cleaned_data <- extracted_lines |>
  mutate(
    # Trim leading and trailing spaces before detection
    trimmed_line = str_trim(extracted_lines),  

    # Detect titles (start with a number and a period)
    is_title = str_detect(trimmed_line, "^\\d+\\. "),  

    # Detect dates (e.g., "Aug 14, 2024")
    is_date = str_detect(trimmed_line, "\\b\\w{3} \\d{1,2}, \\d{4}\\b")
  )

# Step 2: Shift dates to align with corresponding titles
aligned_data <- cleaned_data |>
  mutate(
    date = ifelse(lead(is_date, 1), lead(trimmed_line, 1), NA_character_)  # Shift date to title's row
  ) |>
  filter(is_title) |>
  select(trimmed_line, date)  # Keep only the relevant columns

# Step 3: Rename columns for clarity
final_data <- aligned_data |>
  rename(
    title = trimmed_line,
    date = date
  )

# Step 4: Date and Publication in separate columns, and formatted
final_data <- separate(data = final_data, col = date, into = c("date2", "publication"), sep = "  ", extra = "merge", fill = "right")

# Step 5: Format date, clean headline
final_data <- final_data |> 
  mutate(date = as.Date(date2,format = "%b %d, %Y")) |> 
  mutate(title = str_remove(title, "^\\d+\\. ")) |> 
  subset(select = -(date2)) |> 
  mutate(index = row_number()) |> 
  select(index, date, title, publication)

# Save final data
write_csv(final_data, "./final_data.csv")
```


## Raw text compiler 
```{r, warning=FALSE}
# List all text files in the directory
files <- list.files("~/GitHub/kemi_new/dele_extracted", pattern="*.txt") |>
  as.data.frame() |> 
  rename(filename = 1) |> 
  mutate(
    # Create an index with the file name
    index = str_extract(filename, "\\d+"),
    index = as.numeric(index)
  )

# Join the file list to the final_data dataframe based on index
final_index <- final_data |>
  inner_join(files, c("index")) |>
  mutate(filepath = paste0("~/GitHub/kemi_new/dele_extracted/", filename))

# Display the head of the compiled dataframe with filenames and content
head(final_index)
```


## Text compiler
```{r, warning=FALSE,echo = FALSE, results = 'hide' }
# Define function to loop through each text file 

create_article_text <- function(row_value) {
  temp <- final_index %>%
    slice(row_value)
  
 
  temp_filename <- temp$filename
  
  articles_df_temp <- read_lines(temp$filepath) %>%
    as_tibble() %>%
    mutate(filename = temp_filename)
 
  articles_df <<- articles_df %>%
    bind_rows(articles_df_temp)
}

articles_df <- tibble()
row_values <- 1:nrow(final_index)

lapply(row_values, create_article_text)


# Clean up articles_df and join to index dataframe
articles_df <- articles_df %>%
  select(filename, sentence=value) %>%
  inner_join(final_index)

#After viewing articles_df, I see 1505 lines from the index that I don't need. Cutting them 

articles_df <- articles_df %>%
  slice(-c(1:1505)) |> 
  #gets rid of blank rows
    filter(trimws(sentence) != "") 

write_csv(articles_df, "articles_df.csv")
head(articles_df, 10)
```


## bigrams
```{r, warning=FALSE}
# Load necessary libraries
library(dplyr)
library(tidytext)
library(stringr)

bigrams <- articles_df %>% 
  select(sentence) %>% 
  mutate(
    sentence = str_squish(sentence),                      
    sentence = tolower(sentence),
    sentence = str_replace_all(sentence, c(
    "copyright" = "",
    "new york times"="",
    "publication"="",
    "www.alt"="",
    "http"=""))) %>% 
  unnest_tokens(bigram, sentence, token = "ngrams", n = 2) %>% 
  separate(bigram, c("word1", "word2"), sep = " ") %>% 
  filter(!word1 %in% stop_words$word) %>%                 # Filter out stop words
  filter(!word2 %in% stop_words$word) %>% 
  count(word1, word2, sort = TRUE) %>% 
  filter(!is.na(word1) & !is.na(word2))   
    
# Ensure `stop_words` data is available
data("stop_words")

  # Define the expanded remove pattern for unwanted terms
remove_pattern <- paste(
  "title", "pages", "publication date", "publication subject", "publication type", 
  "issn", "language of publication: english", "document url", "copyright", "news", "lenght", "words", "load", "date", "live updates", "updates assembly", "polling", "units", "total votes",
  "service last updated", "database", "start of article", "rights reserved", 
  "local assembly", "local government", "ups assembly", "live ups", "elections", 
  "accredited voters", "governorship", "registered", "total", "votes", "cast", 
  "valid", "rejected", "polling units", "electoral commission", 
  "progressives congress", "independent", "inec", "presidential", "proquest document id", 
  "classification", "illustration", "caption", "[0-9.]+", "_ftn", "[_]+", "aaa", 
  "jcr:fec", "___________________", "rauchway", "keynes's", "language", "english", 
  "length words", sep = "|"
)

# Process bigrams
bigrams <- articles_df %>% 
  select(sentence) %>% 
  mutate(
    sentence = str_squish(sentence),                      # Remove extra spaces
    sentence = tolower(sentence),                         # Convert to lowercase
    sentence = str_replace_all(sentence, remove_pattern, ""), # Remove unwanted terms
    sentence = str_replace_all(sentence, "- ", ""),       # Remove trailing hyphens
    sentence = str_replace_all(sentence, "\\b[a-zA-Z]\\b", "") # Remove single characters
  ) %>% 
  unnest_tokens(bigram, sentence, token = "ngrams", n = 2) %>% 
  separate(bigram, c("word1", "word2"), sep = " ") %>% 
  filter(!word1 %in% stop_words$word) %>%                 # Filter out stop words
  filter(!word2 %in% stop_words$word) %>% 
  filter(!word1 %in% remove_pattern) %>% 
  count(word1, word2, sort = TRUE) %>% 
  filter(!is.na(word1) & !is.na(word2))                   # Filter out NAs

bigrams
```


## Top 20 bigrams
```{r}
top_bigrams <- bigrams %>%
  arrange(desc(n)) %>%  
  head(20)              
print(top_bigrams)
```


## Visualize top 20 bigrams
```{r}
# Ensure the required libraries are loaded
ggplot(top_bigrams, aes(x = reorder(paste(word1, word2, sep = " "), n), y = n)) +
  geom_col(fill = "steelblue") + 
  coord_flip() +  
  labs(
    title = "Top bigrams from articles",
    caption = "n=454 articles. Graphics by Kemi Busari. Dec 1, 2024",
    x = "Phrases",
    y = "Count of phrases"
  ) +
  theme_minimal(base_size = 14) +
  theme(
    plot.title = element_text(hjust = 0.5, face = "bold", size = 16),
    plot.caption = element_text(hjust = 0, face = "italic", size = 10),
    axis.title.x = element_text(face = "bold"),
    axis.title.y = element_text(face = "bold")
  )
```

## Number of rows - 454
```{r}
nrow(final_data)
```
## Number of columns - 4
```{r}
ncol(final_data)
```

# Sentiment Analysis

```{r, warning=FALSE}
library(textdata)
library(quanteda)
```

## Tokenize text
```{r}
text_tokenized <- articles_df %>% 
  select(sentence) %>% 
  unnest_tokens(word, sentence)
```


## Filter tokenized text
```{r}
text_tokenized <- articles_df %>% 
  select(sentence) %>% 
  mutate(sentence = str_replace_all(sentence, "- ", "")) %>% 
  unnest_tokens(word, sentence) %>% 
  filter(!word %in% stop_words$word) %>% 
  filter(word != "stories_corpus") %>%
  filter(!grepl('[0-9]', word))
```


## Word count
```{r}
text_word_ct <- text_tokenized %>%
  count(word, sort=TRUE)
text_word_ct
```


## Load NRC lexicon
```{r}
nrc_sentiments <- get_sentiments("nrc")

nrc_sentiments %>% count(sentiment)

nrc_sentiments %>% 
  group_by(word) %>% 
  count() %>% 
  arrange(desc(n)) %>% 
  distinct()
```

## Count Overall Sentiment with NRC
```{r}

sentiments_all <- text_tokenized %>%
  inner_join(nrc_sentiments) 

sentiments_all %>% 
  group_by(word) %>% 
    count(sentiment) %>% 
  arrange(desc(n))
```

```{r}
sentiments_all <- text_tokenized %>%
  inner_join(nrc_sentiments) %>%
  count(sentiment, sort = TRUE) %>% 
  mutate(pct_total =round(n/sum(n), digits=2))

sentiments_all
```



## Create ggplot
```{r}
# Load necessary libraries
library(ggplot2)
library(dplyr)

# Plot the sentiment analysis results
nrc_plot <- sentiments_all %>% 
  ggplot(aes(x = reorder(sentiment, n), y = n, fill = sentiment)) + 
  geom_bar(stat = "identity", position = "dodge") +  
  theme(legend.position = "none") +  
  labs(
    title = "Total Sentiment in Articles About Dele Giwa",
    subtitle = " ",
    caption = "NRC Sentiment Analysis. Graphic by Kemi Busari, Dec 1, 2024",
    x = "Total Sentiment Score",
    y = "Score"
  ) +
  theme_minimal(base_size = 14) +
  theme(
    plot.title = element_text(face = "bold", size = 16),
    plot.caption = element_text(hjust = 0, size = 10)
  ) +
  scale_fill_brewer(palette = "Set3") +
  coord_flip()  

# Print the plot
print(nrc_plot)
```

## Check the words in some of the sentiments

# Positive
```{r}
nrc_positive <- nrc_sentiments %>%
  filter(sentiment == "positive")
dele_positive <- text_tokenized %>%
  inner_join(nrc_positive) %>%
  count(word, sort = TRUE)
dele_positive

```

# Trust
```{r}
#Trust
nrc_trust <- nrc_sentiments %>%
  filter(sentiment == "trust")
dele_trust <- text_tokenized %>%
  inner_join(nrc_trust) %>%
  count(word, sort = TRUE)
dele_trust
```


# Negative
```{r}
nrc_negative <- nrc_sentiments %>%
  filter(sentiment == "negative")
dele_negative <- text_tokenized %>%
  inner_join(nrc_negative) %>%
  count(word, sort = TRUE)
dele_negative
```


# Fear
```{r}
nrc_fear <- nrc_sentiments %>%
  filter(sentiment == "fear")
dele_fear <- text_tokenized %>%
  inner_join(nrc_fear) %>%
  count(word, sort = TRUE)
dele_fear
```


# Anticipation
```{r}
nrc_anticipation <- nrc_sentiments %>%
  filter(sentiment == "anticipation")
dele_anticipation <- text_tokenized %>%
  inner_join(nrc_anticipation) %>%
  count(word, sort = TRUE)
dele_anticipation
```


# Anger
```{r}
nrc_anger <- nrc_sentiments %>%
  filter(sentiment == "anger")
dele_anger <- text_tokenized %>%
  inner_join(nrc_anger) %>%
  count(word, sort = TRUE)
dele_anger
```

## Sentiment during military era - 1986-1999
```{r}
#military_sentiment <-  articles_df %>% 
  #filter(year >= 1986 & year <=1999)
```

## Sentiment in the democratic era - 2000 -2023
```{r}
#military_sentiment <-  articles_df %>% 
  #filter(year >= 2000 & year <=2023)
```

Note: These two codes did not work because my final dataframe did not capture year and publication. I am still trying to figure this out.



# Topic modelling

#Install and load packages 
```{r, warning=FALSE}
# install.packages("here")
# install.packages("tidytext")
# install.packages("quanteda")
# install.packages("tm")
# install.packages("topicmodels")
# install.packages("reshape2")
# install.packages("ggplot2")
# install.packages("wordcloud")
# install.packages("pals")
# install.packages("SnowballC")
# install.packages("lda")
# install.packages("ldatuning")
# install.packages("kableExtra")
# install.packages("DT")
# install.packages("flextable")
# install.packages("remotes")
# remotes::install_github("rlesur/klippy")
#install.packages("rio")
#install.packages("readtext")
#install.packages("formattable")


```


```{r include=FALSE}
# set options
options(stringsAsFactors = F)         # no automatic data transformation
options("scipen" = 100, "digits" = 4) # suppress math annotation
# load packages
here::here()
library(tidyverse)
library(tidytext)
library(rio)
library(readtext)
#topic modeling
library(quanteda)
library(tm)
library(topicmodels)
library(lda)
library(ldatuning)
# from tutorial packages
library(DT)
library(knitr) 
library(kableExtra) 
library(reshape2)
library(ggplot2)
library(wordcloud)
library(pals)
library(SnowballC)
library(flextable)
```


## Process into corpus object
```{r}
topic_data <- articles_df %>% 
  select(filename, sentence) %>% 
  as.data.frame() %>% 
  rename(doc_id = filename, text= sentence)

# load stopwords
english_stopwords <- readLines("https://slcladal.github.io/resources/stopwords_en.txt", encoding = "UTF-8")
# create corpus object
corpus <- Corpus(DataframeSource(topic_data))
# Preprocessing chain
processedCorpus <- tm_map(corpus, content_transformer(tolower))
processedCorpus <- tm_map(processedCorpus, removeWords, english_stopwords)
processedCorpus <- tm_map(processedCorpus, removePunctuation, preserve_intra_word_dashes = TRUE)
processedCorpus <- tm_map(processedCorpus, removeNumbers)
processedCorpus <- tm_map(processedCorpus, stemDocument, language = "en")
processedCorpus <- tm_map(processedCorpus, stripWhitespace)
```



```{r tm3a}
#DTM: rows correspond to the documents in the corpus. Columns correspond to the terms in the documents. Cells correspond to the weights of the terms. (Girder)
# compute document term matrix with terms >= minimumFrequency
minimumFrequency <- 5
DTM <- DocumentTermMatrix(processedCorpus, control = list(bounds = list(global = c(minimumFrequency, Inf))))
# have a look at the number of documents and terms in the matrix
dim(DTM)
# due to vocabulary pruning, we have empty rows in our DTM
# LDA does not like this. So we remove those docs from the
# DTM and the metadata
sel_idx <- slam::row_sums(DTM) > 0
DTM <- DTM[sel_idx, ]
topic_data <- topic_data[sel_idx, ]
#5 term minimum[1] 1387 3019
#5 term minimum[1] 308597 10339

``` 

## Topic proportions 

```{r, warning=FALSE}
# number of topics
# K <- 20
K <- 6
# set random number generator seed
set.seed(9161)
#Latent Dirichlet Allocation, LDA
topicModel2 <- LDA(DTM, K, method="Gibbs", control=list(iter = 1000, verbose = 25, alpha = 0.2))
tmResult <- posterior(topicModel2)
theta <- tmResult$topics
beta <- tmResult$terms
topicNames <- apply(terms(topicModel2, 10), 2, paste, collapse = " ")  # reset topicnames
```

## Number of articles
```{r}
# Step 1: Check dimensions
n_theta <- nrow(theta)
n_topicdata<- length(topic_data)

cat("Number of rows in theta: ", n_theta, "\n")
cat("Number of documents in textdata: ", n_topicdata, "\n")

# Check if textdata contains all the documents in theta
common_ids <- intersect(rownames(theta), topic_data$doc_id) # Assuming textdata has a 'doc_id' column

# Filter textdata to include only the documents present in theta
topicdata_filtered <- topic_data[topic_data$doc_id %in% common_ids, ]

# Check dimensions after filtering
n_topicdata_filtered <- nrow(topicdata_filtered)
cat("Number of documents in filtered textdata: ", n_topicdata_filtered, "\n")


# Align rownames of theta with filtered textdata
theta_aligned <- theta[rownames(theta) %in% topicdata_filtered$doc_id, ]

# Step 2: Combine data
full_data <- data.frame(theta_aligned, decade = topicdata_filtered)

# get mean topic proportions per decade
# topic_proportion_per_decade <- aggregate(theta, by = list(decade = textdata$decade), mean)
# set topic names to aggregated columns
colnames(full_data)[2:(K+1)] <- topicNames
# reshape data frame
vizDataFrame <- melt(full_data)
   
```

## Examine topic

```{r}
#enframe(): Converts a named list into a dataframe.
topics <- enframe(topicNames, name = "number", value = "text") %>% 
  unnest(cols = c(text)) 
  
topics
```

## Review the topics 
```{r}
theta2 <- as.data.frame(theta)

topic1 <- theta2 %>% 
  rownames_to_column(var = "file") |> # putting the rownames into a new column called file
  mutate(file = str_remove(file, "^X"),  # Remove leading 'X'
         line = str_extract(file, "(?<=\\.txt)\\.\\d+")) |>   # Extract number after .txt
  mutate(file = str_remove(file, "\\.\\d+$")) |> 
  rename(topic1 = '1') |> 
  top_n(20, topic1) |> 
  arrange(desc(topic1)) |>  
  select(file, line, topic1) 
```


## Add categories

```{r}
vizDataFrame <- vizDataFrame %>% 
  mutate(category = case_when(
    str_detect(variable, "govern presid countri nigeria militari polit peopl court nigerian nation") ~ "unresolved_case",
    str_detect(variable, "giwa dele journalist kill murder polic death bomb lago load-dat") ~ "assasination",
    str_detect(variable, "book journal school write ray read year univers nigerian stori") ~ "remenbrance",
    str_detect(variable, "page media nigeria newspap report nation editor right time news") ~ "nigerian_newspapers",
    str_detect(variable, "state vote elect pdp apc govern voter parti word unit") ~ "election",
    str_detect(variable, "peopl man god life day time good thing nigeria live
") ~ "memory",
    TRUE ~ NA_character_  
  ))

```

Note: I reached a dead end here and would love to get some guidance.



# Statistics from Excel data



# load excel file and clean date column
```{r, warning=FALSE}
library(readxl)
library(lubridate)
dele_excel  <- rio::import("dele_list.XLSX") %>%
  janitor::clean_names()
# Remove the days of the week from the `publication_date` column
# Convert the publication_date to Date format and remove weekdays (e.g., "Monday", "Tuesday")
dele_excel <- dele_excel %>%
  mutate(
    published_date = str_remove(published_date, "^\\s*(Monday|Tuesday|Wednesday|Thursday|Friday|Saturday|Sunday),?\\s*") %>%
    mdy(),  # Adjust mdy() if your date format is MM/DD/YYYY, use dmy() for DD/MM/YYYY
    # Create a new column `year` by extracting the year from the `publication_date`
    year = year(published_date)
  ) %>%
  # Arrange the dataframe in descending order by publication_date
  arrange(desc(published_date))

# View the cleaned data
head(dele_excel)
```



## Count the number of articles published per year
```{r}
# Count the number of articles published per year and arrange by year
articles_per_year <- dele_excel %>%
  count(year) %>%  
  arrange(year)    

# View the count of articles per year
print(articles_per_year)
```

## Remove NA values from the year column
```{r}
dele_excel <- dele_excel %>%
  filter(!is.na(year))  
```


## visualize year
```{r}
ggplot(articles_per_year, aes(x = year, y = n)) +
  geom_line(color = "steelblue", size = 1.2) +  
  geom_point(color = "black", size = 3) +         
  labs(
    title = "Number of Articles per Year",
    subtitle = "Date Created: Nov 23, 2024",
    caption = "Graph by Kemi Busari",
    x = "Year",
    y = "Number of Articles"
  ) +
  theme_minimal() +  
  theme(
    plot.title = element_text(size = 16, face = "bold"),
    plot.subtitle = element_text(size = 12),
    plot.caption = element_text(size = 10, face = "italic"),
    axis.title = element_text(size = 12),
    axis.text = element_text(size = 10)
  )
```


## Identify the top five years with the most publications
```{r}
top_5 <- articles_per_year %>%
  top_n(5, n)
```


## Visualize top 5
```{r, warning=FALSE}
ggplot(top_5, aes(x = factor(year), y = n)) +
  geom_bar(stat = "identity", fill = "steelblue", color = "black") +
  labs(
    title = "Top 5 Years with Most Publications",
    subtitle = "Date Created: Nov 23, 2024",
    caption = "Graph by Kemi Busari",
    x = "Year",
    y = "Number of Articles"
  ) 
```

## count the number of articles in military era(1986 to 1999)
```{r}
articles_1986_1999 <- dele_excel %>%
  filter(year >= 1986 & year <= 1999) %>%  
  count(year)
```
#Only 14 articles were published during the military era


## count the number of articles in the democratic era(2000 to 2023)
```{r}
articles_2000_2023 <- dele_excel %>%
  filter(year >= 2000 & year <= 2023) %>% 
  count(year) 
total_articles_2000_2023 <- sum(articles_2000_2023$n)
```


## Remove content inside brackets in publication column
```{r}
dele_excel <- dele_excel %>%
  mutate(publication_4 = str_remove_all(publication_4, "\\(.*?\\)"))
```


## Merge "Weekly Trust" into "Daily Trust", they are the same
```{r}
dele_excel <- dele_excel %>%
  mutate(publication_4 = str_replace(publication_4, "Weekly Trust", "Daily Trust"))
```


## Count the number of articles published by each newspaper
```{r}
articles_per_newspaper <- dele_excel %>%
  count(publication_4) %>%  
  group_by(publication_4) %>%  
  summarise(total_articles = sum(n)) %>%  
  arrange(desc(total_articles)) 
```
43 news platforms published about Dele Giwa


## Show top 10 newspapers
```{r}
top_10_newspapers <- articles_per_newspaper %>%
  arrange(desc(total_articles)) %>%
  head(10)
```


## Visualize top 10
```{r}
ggplot(top_10_newspapers, aes(x = reorder(publication_4, total_articles), y = total_articles)) +
  geom_bar(stat = "identity", fill = "steelblue", color = "black") +  
  geom_text(aes(label = total_articles), vjust = -0.3, size = 5, color = "black") +  
  labs(
    title = "Top 10 Newspapers with Most Articles",
    subtitle = "Date Created: Nov 23, 2024",
    caption = "Graph by Kemi Busari",
    x = "Newspaper",
    y = "Number of Articles"
  ) +
  theme_minimal() +  
  theme(
    axis.text.x = element_text(angle = 60, hjust = 1)
  )
```

## Lessons and what next?

1. Data needs more cleaning
2. Need to figure out how to include values for date and publication in the final dataframe
3. Need to complete topic modelling
4. Open to suggestions to improve







#Summary of final project (word version uploaded to directory)
In morning of Monday, October 20, 1986, Dele Giwa, regarded as one of the most critical investigative journalists in Nigeria, was assassinated at his residence in Lagos, Nigeria. While the case is still unresolved, Nigerian journalists at every anniversary (and outside the period) publish news and editorialized articles to commemorate the assassination of Giwa. This study examines these commemorative articles to identify the dominant themes. The study also analyzes differences in the commemoration during the military era (1987 to May 1999) and civilian (1999 to 2023). Inquiry will be guided by these research questions:
  
#Research Questions
1.	What are the dominant themes in the articles commemorating Dele Giwa’s assassination?
2.	What are the differences in the commemoration of Dele Giwa between the military era (Oct 1987 to May 1999) and the civilian era (May 1999 to Oct 2023)?
```



```{r}
#Spell out your content analysis plan, including preliminary work on a code book
#The four hundred and fifty four articles were loaded into R, cleaned and prepared for analysis. The unit of analysis are articles about Dele Giwa. The following analysis will be run with the ultimate aim of identifying dominant themes:
#Bigrams: The dataset will be tokenized and then analyzed for bigrams to reveal the top phrases.
#Topic modelling: Latent Dirichlet Allocation (LDA) will be used to cluster words into topics. This will also be used to examine how the themes evolve over time (military vs. civilian era).
#Sentiment analysis: The Afinn dictionary will be used to analyze the overall tone of the articles. (Not sure about this yet because I think this won’t bring anything special)

#Coding scheme
#Below are some preliminary thoughts on a coding scheme based on the literature and my familiarization with the data:
#Bravery and Sacrifice: Mentions of Dele Giwa’s courage, investigative prowess, or ultimate sacrifice for truth and justice.
#Calls for Justice: different expressions advocating for justice or accountability for his assassination.
#Criticism of Authorities: Critical reflections on the failure of successive governments, law enforcement, or judiciary to solve the murder will fall under this category
#Freedom of the Press: Themes highlighting press freedom and the risks journalists face in Nigeria.
#Comparison of Eras: Contrasts between the military and civilian eras in terms of press freedom, accountability, or socio-political environment.
#Role Model: Description of Dele Giwa as an icon of good journalism whose work inspires other journalists.
```